{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "from utils import cyclical_encoder\n",
    "from utils import train_val_test_split\n",
    "from utils import train_val_test_split_np\n",
    "from utils import get_model_metrics\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#xgboost\n",
    "from xgboost import XGBRegressor as xgbr\n",
    "from xgboost import XGBRFRegressor as xgbrfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(resolution, nan_value, val_days, test_days):\n",
    "    \n",
    "    if resolution == '16x10':\n",
    "        path = '/home/tjarke/Desktop/Residual_Load/work/Feature_channels/'\n",
    "        file = f'feature_channel_{resolution}.npy'\n",
    "        feature_channel = np.load(path+file, allow_pickle=False)\n",
    "\n",
    "        # get rid of all nans - replace them with nan_value\n",
    "        feature_channel = np.nan_to_num(feature_channel, nan=nan_value)\n",
    "        \n",
    "        # rearange the axis to get the examples as the first dimension for the use in tensorflow\n",
    "        feature_channel = np.moveaxis(feature_channel, -1, 0)\n",
    "        feature_channel_load = feature_channel[:,:,:,:7]\n",
    "        print(f'The shape of the feature_channel_load is {feature_channel_load.shape}')\n",
    "        feature_channel_gen = feature_channel       \n",
    "        print(f'The shape of the feature_channel_gen is {feature_channel_gen.shape}')\n",
    "        \n",
    "    else:\n",
    "        path = '/home/tjarke/Desktop/Residual_Load/work/Feature_channels/'\n",
    "        weather_file = f'feature_channel_{resolution}.npy'\n",
    "        feature_channel_load = np.load(path+weather_file, allow_pickle=False)\n",
    "        feature_channel_load = np.nan_to_num(feature_channel_load, nan=nan_value)        \n",
    "        feature_channel_load = np.moveaxis(feature_channel_load, -1, 0)\n",
    "        print(f'The shape of the feature_channel_load is {feature_channel_load.shape}')\n",
    "        \n",
    "        path = '~/Desktop/Residual_Load/work/Feature_channels/'\n",
    "        ic_file = f'installed_capacities_{resolution}.npy'\n",
    "        ic_channel = np.load(path+ic_file, allow_pickle=False)\n",
    "        ic_channel = np.moveaxis(ic_channel, -1, 0)\n",
    "        feature_channel_gen = np.dstack((feature_channel_load, ic_channel))\n",
    "        print(f'The shape of the feature_channel_gen is {feature_channel_gen.shape}')\n",
    "\n",
    "        \n",
    "    # load the holidays in germany\n",
    "    path = '/home/tjarke/Desktop/Residual_Load/work/'\n",
    "    hol_file = 'holidays_encoded.csv'\n",
    "    df_hol = pd.read_csv(path+hol_file,parse_dates=[0])\n",
    "    print(f'The df_hol has the shape {df_hol.shape}')\n",
    "        \n",
    "    # load the realised values\n",
    "    path = '~/Desktop/Residual_Load/work/'\n",
    "    file_realised = 'Day_ahead_dataset.csv'\n",
    "    df_realised = pd.read_csv(path+file_realised,parse_dates=[1])\n",
    "    df_realised.drop(columns=[\"index\"],inplace=True)\n",
    "    # fill all solar nan with zero (even though some times it is during the day)\n",
    "    df_realised.loc[df_realised['Realised/Solar in MAW'].isnull(),'Realised/Solar in MAW']  = 0\n",
    "    # fill all other nans with interpolat\n",
    "    df_realised = df_realised.interpolate(method='linear')\n",
    "    \n",
    "#     df_entsoe = df_realised.loc[:,['Day ahead/System total load in MAW', 'Day ahead/Solar in MAW',\n",
    "#        'Day ahead/Wind Onshore in MAW', 'Day ahead/Wind Offshore in MAW']].copy()\n",
    "    \n",
    "#     df_realised.drop(columns=['index', 'Day ahead/System total load in MAW', 'Day ahead/Solar in MAW',\n",
    "#        'Day ahead/Wind Onshore in MAW', 'Day ahead/Wind Offshore in MAW'], inplace=True)\n",
    "    # get rid of nans\n",
    "#     df_realised = df_realised.interpolate(method='linear')\n",
    "    print(f'The shape of the df_realised is {df_realised.shape}')\n",
    "\n",
    "    \n",
    "    target_vars = [\"Realised/System total load in MAW\",\n",
    "                   \"Realised/Wind Offshore in MAW\",\n",
    "                   \"Realised/Wind Onshore in MAW\",\n",
    "                   \"Realised/Solar in MAW\"]\n",
    "    \n",
    "    # Normalize the data\n",
    "#     X_max = 100_000\n",
    "#     for col in target_vars:\n",
    "#         df_realised[col] /= X_max\n",
    "    \n",
    "    # create time columns\n",
    "    \n",
    "    df_realised['Year'] = df_realised['Date'].dt.year-2014    \n",
    "    df = cyclical_encoder(df_realised,1440,\"minute\")\n",
    "    df = cyclical_encoder(df,12,\"month\")\n",
    "    df = cyclical_encoder(df,7,\"weekday\")\n",
    "\n",
    "        \n",
    "    # kill the last 100 rows since these are nan values\n",
    "    # skip the first year\n",
    "#     n_start = 0\n",
    "    n_start = int(96*365*0)\n",
    "    num_rows = -100\n",
    "    df = df.iloc[n_start:num_rows,:]\n",
    "#     df_entsoe = df_entsoe.iloc[:num_rows,:]\n",
    "    df_hol = df_hol.iloc[n_start:-8,:]\n",
    "    feature_channel_gen = feature_channel_gen[n_start:num_rows,:,:,:]\n",
    "    feature_channel_load = feature_channel_load[n_start:num_rows,:,:,:]\n",
    "    print(f'After getting rid of the last 100 rows and startin after {n_start} steps the shapes are:')\n",
    "    print(f'df_realised {df.shape}, df_hol {df_hol.shape}, fc_gen {feature_channel_gen.shape}') \n",
    "    print(f'fc_load {feature_channel_load.shape}')\n",
    "    \n",
    "    # Split the data into train, val, test\n",
    "    X_train_day, X_train_realised, \\\n",
    "    X_val_day, X_val_realised, \\\n",
    "    X_test_day, X_test_realised = train_val_test_split(df, target_vars, val_days, test_days)\n",
    "    \n",
    "    y_true = X_test_realised.copy()\n",
    "    X_test_realised.drop(columns='Date', inplace=True)\n",
    "    \n",
    "    print(f'\\nThe order of the columns for X_realised is: {X_train_realised.columns}')\n",
    "#     print(f'The order of the columns for X_realised is: {X_val_realised.columns}')\n",
    "#     print(f'The order of the columns for X_realised is: {X_test_realised.columns}\\n')\n",
    "    \n",
    "    print(f'\\nThe order of the columns for X_day is: {X_train_day.columns}')\n",
    "#     print(f'The order of the columns for X_realised is: {X_val_realised.columns}')\n",
    "#     print(f'The order of the columns for X_realised is: {X_test_realised.columns}\\n')\n",
    "    \n",
    "    y_train = X_train_realised.to_numpy(copy=True)\n",
    "    y_val = X_val_realised.to_numpy(copy=True)\n",
    "    X_train_day = X_train_day.to_numpy(copy=True)\n",
    "    X_train_realised = X_train_realised.to_numpy(copy=True)\n",
    "    X_val_day = X_val_day.to_numpy(copy=True)\n",
    "    X_val_realised = X_val_realised.to_numpy(copy=True)\n",
    "    X_test_day = X_test_day.to_numpy(copy=True)\n",
    "    X_test_realised = X_test_realised.to_numpy(copy=True)\n",
    "    \n",
    "    # split the feature channels into train, val, test\n",
    "    X_train_fc_gen, X_val_fc_gen, X_test_fc_gen = train_val_test_split_np(feature_channel_gen, \n",
    "                                                                           val_days, test_days)\n",
    "\n",
    "    X_train_fc_load, X_val_fc_load, X_test_fc_load = train_val_test_split_np(feature_channel_load, \n",
    "                                                                              val_days, test_days)\n",
    "    \n",
    "    # pre-process the holidays and split it into train, val and test\n",
    "    df_hol['holidays'] = df_hol.mean(axis=1)\n",
    "    df_hol.rename(columns={'time':'Date'},inplace=True)\n",
    "    cols = [col for col in df_hol.columns if col not in ['holidays']]\n",
    "#     df_hol['Date'] = 0 # so that the train, val, test split works  \n",
    "    X_train_hol, _, X_val_hol, _, X_test_hol, _ = train_val_test_split(df_hol, cols, val_days, test_days)\n",
    "    \n",
    "    X_train_hol = X_train_hol.to_numpy(copy=True)\n",
    "    X_val_hol = X_val_hol.to_numpy(copy=True)\n",
    "    X_testn_hol = X_test_hol.to_numpy(copy=True)\n",
    "    \n",
    "    y_test = y_true.iloc[:, :-1].to_numpy(copy=True)\n",
    "    \n",
    "    return y_train, y_val, y_true, y_test,\\\n",
    "           X_train_realised, X_val_realised, X_test_realised, \\\n",
    "           X_train_day, X_val_day, X_test_day, \\\n",
    "           X_train_hol, X_val_hol, X_test_hol, \\\n",
    "           X_train_fc_gen, X_val_fc_gen, X_test_fc_gen, \\\n",
    "           X_train_fc_load, X_val_fc_load, X_test_fc_load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature_channel_load is (204096, 16, 10, 7)\n",
      "The shape of the feature_channel_gen is (204096, 16, 10, 9)\n",
      "The df_hol has the shape (204004, 17)\n",
      "The shape of the df_realised is (204096, 9)\n",
      "After getting rid of the last 100 rows and startin after 0 steps the shapes are:\n",
      "df_realised (203996, 16), df_hol (203996, 17), fc_gen (203996, 16, 10, 9)\n",
      "fc_load (203996, 16, 10, 7)\n",
      "The shape of the data set is: (203996, 16)\n",
      "--------------------------------------------\n",
      "The shape of the train set is: (186716, 11)\n",
      "The shape of the target variable is: (186716, 4)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the validation set is: (8640, 11)\n",
      "The shape of the target variable for the validation set is: (8640, 4)\n",
      "--------------------------------------------\n",
      "\n",
      "--------------------------------------------\n",
      "The shape of the test set is: (8640, 11)\n",
      "The shape of the target variable for the test set is: (8640, 5)\n",
      "--------------------------------------------\n",
      "\n",
      "The order of the columns for X_realised is: Index(['Realised/System total load in MAW', 'Realised/Wind Offshore in MAW',\n",
      "       'Realised/Wind Onshore in MAW', 'Realised/Solar in MAW'],\n",
      "      dtype='object')\n",
      "\n",
      "The order of the columns for X_day is: Index(['Day ahead/System total load in MAW', 'Day ahead/Solar in MAW',\n",
      "       'Day ahead/Wind Onshore in MAW', 'Day ahead/Wind Offshore in MAW',\n",
      "       'Year', 'sin_minute', 'cos_minute', 'sin_month', 'cos_month',\n",
      "       'sin_weekday', 'cos_weekday'],\n",
      "      dtype='object')\n",
      "The shape of the feature data set is: (203996, 16, 10, 9)\n",
      "--------------------------------------------\n",
      "The shape of the train set is: (186716, 16, 10, 9)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the validation set is: (8640, 16, 10, 9)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the test set is: (8640, 16, 10, 9)\n",
      "--------------------------------------------\n",
      "The shape of the feature data set is: (203996, 16, 10, 7)\n",
      "--------------------------------------------\n",
      "The shape of the train set is: (186716, 16, 10, 7)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the validation set is: (8640, 16, 10, 7)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the test set is: (8640, 16, 10, 7)\n",
      "--------------------------------------------\n",
      "The shape of the data set is: (203996, 18)\n",
      "--------------------------------------------\n",
      "The shape of the train set is: (186716, 1)\n",
      "The shape of the target variable is: (186716, 17)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "The shape of the validation set is: (8640, 1)\n",
      "The shape of the target variable for the validation set is: (8640, 17)\n",
      "--------------------------------------------\n",
      "\n",
      "--------------------------------------------\n",
      "The shape of the test set is: (8640, 1)\n",
      "The shape of the target variable for the test set is: (8640, 18)\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_train, y_val, y_true, y_test,\\\n",
    "X_train_realised, X_val_realised, X_test_realised, \\\n",
    "X_train_day, X_val_day, X_test_day, \\\n",
    "X_train_hol, X_val_hol, X_test_hol, \\\n",
    "X_train_fc_gen, X_val_fc_gen, X_test_fc_gen, \\\n",
    "X_train_fc_load, X_val_fc_load, X_test_fc_load = load_data('16x10', 0, 90, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the y_sets shape: (186716, 4), (8640, 4), (8640, 5)\n",
      "the realised sets shape: (186716, 4), (8640, 4), (8640, 4)\n",
      "the day set shape: (186716, 11), (8640, 11), (8640, 11)\n",
      "the fc_gen shape: (186716, 16, 10, 9), (8640, 16, 10, 9), (8640, 16, 10, 9)\n",
      "the fc_load shape: (186716, 16, 10, 7), (8640, 16, 10, 7), (8640, 16, 10, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f'the y_sets shape: {y_train.shape}, {y_val.shape}, {y_true.shape}')\n",
    "print(f'the realised sets shape: {X_train_realised.shape}, {X_val_realised.shape}, {X_test_realised.shape}')\n",
    "print(f'the day set shape: {X_train_day.shape}, {X_val_day.shape}, {X_test_day.shape}')\n",
    "print(f'the fc_gen shape: {X_train_fc_gen.shape}, {X_val_fc_gen.shape}, {X_test_fc_gen.shape}')\n",
    "print(f'the fc_load shape: {X_train_fc_load.shape}, {X_val_fc_load.shape}, {X_test_fc_load.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a better generator function:\n",
    "\n",
    "def generator(recognizer, \n",
    "              time_steps_weather, time_steps_realised, time_steps_days,\n",
    "              feature_matrix_fc_gen, feature_matrix_fc_load, \n",
    "              feature_matrix_day, feature_matrix_hol,\n",
    "              feature_matrix_realised, target_matrix):\n",
    "    '''\n",
    "    INPUT:\n",
    "    recognizer: A string: either 'train', 'val' or 'test'. Make sure each variable gets a unique name\n",
    "    time_steps_weather: A list: list of time steps to look in the past for the weather data/feature_channel, \n",
    "    e.g. 4 means get the value from one hour ago\n",
    "    time_steps_realised: same as before, but for the realised/actual values\n",
    "    feature_matrix_fc: numpy array with the information for the feature_channel/weather data\n",
    "    feature_matrix_lag: same as before, but for the realised/actual data\n",
    "    target_matrix: numpy array witht the target data\n",
    "    '''\n",
    "    max_steps_weather = max(time_steps_weather)\n",
    "    max_steps_realised = max(time_steps_realised)\n",
    "    max_steps_days = max(time_steps_days)\n",
    "    \n",
    "    # create dictionaries of lists for the features and a list for the target:\n",
    "    steps_weather_gen_dict = dict()\n",
    "    steps_weather_load_dict = dict()\n",
    "    steps_realised_dict = dict()\n",
    "    steps_days_dict = dict()\n",
    "    steps_y_target_dict = dict()\n",
    "   \n",
    "\n",
    "    \n",
    "    # concatenate feature_matrix_realised and feature_matrix_day and hol so that the lags have the time info\n",
    "    # as well\n",
    "#     feature_matrix_realised = np.concatenate((feature_matrix_realised,\n",
    "#                                               feature_matrix_day,\n",
    "#                                               feature_matrix_hol), axis=1)\n",
    "#     print(f'feature_matrix_realised shape after concat is {feature_matrix_realised.shape}')\n",
    "    \n",
    "    # concatenate feature_matrix_day and feature_matrix_hol \n",
    "    days_matrix = np.concatenate((feature_matrix_day, feature_matrix_hol), axis=1)\n",
    "    \n",
    "    for step in time_steps_weather:\n",
    "        steps_weather_gen_dict[f'{recognizer}_weather_gen_step_{step}'] = list()\n",
    "\n",
    "    for step in time_steps_weather:\n",
    "        steps_weather_load_dict[f'{recognizer}_weather_load_step_{step}'] = list()\n",
    "        \n",
    "    for step in time_steps_realised:\n",
    "        steps_realised_dict[f'{recognizer}_realised_step_{step}'] = list()\n",
    "    \n",
    "    for step in time_steps_days:\n",
    "        steps_days_dict [f'{recognizer}_days_step_{step}'] = list()\n",
    "        \n",
    "    for step in time_steps_days:\n",
    "        steps_y_target_dict[f'{recognizer}_y_target_step_{step}'] = list()\n",
    "        \n",
    "        \n",
    "    # iterate through each element of feature_matrix_fc_gen\n",
    "    for i in range(len(target_matrix)):\n",
    "        end_ix = i + max(max_steps_weather, max_steps_realised, max_steps_days)\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+1 > len(target_matrix):\n",
    "            break\n",
    "        for step in time_steps_weather:\n",
    "            steps_weather_gen_dict[f'{recognizer}_weather_gen_step_{step}'].append(feature_matrix_fc_gen[end_ix - step])\n",
    "\n",
    "    # iterate through each element of feature_matrix_fc_load\n",
    "    for i in range(len(target_matrix)):\n",
    "        end_ix = i + max(max_steps_weather, max_steps_realised, max_steps_days)\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+1 > len(target_matrix):\n",
    "            break\n",
    "        for step in time_steps_weather:\n",
    "            steps_weather_load_dict[f'{recognizer}_weather_load_step_{step}'].append(feature_matrix_fc_load[end_ix - step])\n",
    "    \n",
    "    # iterate through each element of feature_matrix_realised\n",
    "    for i in range(len(target_matrix)):\n",
    "        end_ix = i + max(max_steps_weather, max_steps_realised, max_steps_days)\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+1 > len(target_matrix):\n",
    "            break\n",
    "        for step in time_steps_realised:\n",
    "            steps_realised_dict[f'{recognizer}_realised_step_{step}'].append(feature_matrix_realised[end_ix - step])\n",
    "            \n",
    "    for i in range(len(target_matrix)):\n",
    "        end_ix = i + max(max_steps_weather, max_steps_realised, max_steps_days)\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+1 > len(target_matrix):\n",
    "            break\n",
    "        for step in time_steps_days:\n",
    "            steps_days_dict[f'{recognizer}_days_step_{step}'].append(days_matrix[end_ix - step])\n",
    "            steps_y_target_dict[f'{recognizer}_y_target_step_{step}'].append(target_matrix[end_ix - step])\n",
    "            \n",
    "#         seq_y_target = target_matrix[end_ix]\n",
    "#         seq_day = days_matrix[end_ix]\n",
    "#         y_target.append(seq_y_target)\n",
    "#         days.append(seq_day)\n",
    "        \n",
    "    # transform the lists in the dictionaries to numpy arrays     \n",
    "    for key in steps_weather_gen_dict.keys():\n",
    "        steps_weather_gen_dict[key] = np.array(steps_weather_gen_dict[key])\n",
    "        \n",
    "    for key in steps_weather_load_dict.keys():\n",
    "        steps_weather_load_dict[key] = np.array(steps_weather_load_dict[key])\n",
    "        \n",
    "    for key in steps_realised_dict.keys():\n",
    "        steps_realised_dict[key] = np.array(steps_realised_dict[key])\n",
    "        \n",
    "    for key in steps_days_dict.keys():\n",
    "        steps_days_dict[key] = np.array(steps_days_dict[key])\n",
    "\n",
    "    for key in steps_y_target_dict.keys():\n",
    "        steps_y_target_dict[key] = np.array(steps_y_target_dict[key])\n",
    "\n",
    "        \n",
    "\n",
    "    return steps_weather_gen_dict, steps_weather_load_dict, steps_realised_dict, steps_days_dict, steps_y_target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_weather = [0]\n",
    "time_steps_realised = np.arange(1,5,1).tolist()\n",
    "time_steps_days = [0] #np.arange(1,97,1).tolist()\n",
    "\n",
    "X_train_fc_gen, X_train_fc_load, X_train_realised, X_train_days, y_train = generator('train', \n",
    "                                                                                     time_steps_weather, \n",
    "                                                                                     time_steps_realised,\n",
    "                                                                                     time_steps_days,\n",
    "                                                                                     X_train_fc_gen, \n",
    "                                                                                     X_train_fc_load,\n",
    "                                                                                     X_train_day,\n",
    "                                                                                     X_train_hol,\n",
    "                                                                                     X_train_realised,\n",
    "                                                                                     y_train)\n",
    "\n",
    "\n",
    "X_val_fc_gen, X_val_fc_load, X_val_realised, X_val_days, y_val = generator('val', \n",
    "                                                                             time_steps_weather, \n",
    "                                                                             time_steps_realised,\n",
    "                                                                             time_steps_days, \n",
    "                                                                             X_val_fc_gen, \n",
    "                                                                             X_val_fc_load,\n",
    "                                                                             X_val_day,\n",
    "                                                                             X_val_hol,\n",
    "                                                                             X_val_realised,\n",
    "                                                                             y_val)\n",
    "\n",
    "\n",
    "X_test_fc_gen, X_test_fc_load, X_test_realised, X_test_days, y_test = generator('test', \n",
    "                                                                                 time_steps_weather, \n",
    "                                                                                 time_steps_realised,\n",
    "                                                                                 time_steps_days, \n",
    "                                                                                 X_test_fc_gen, \n",
    "                                                                                 X_test_fc_load,\n",
    "                                                                                 X_test_day,\n",
    "                                                                                 X_test_hol,\n",
    "                                                                                 X_test_realised,\n",
    "                                                                                 y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186712, 4, 4) (8636, 4, 4) (8636, 4, 4)\n",
      "(186712, 1, 12) (8636, 1, 12) (8636, 1, 12)\n",
      "(186712, 1, 4) (8636, 1, 4) (8636, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# realised data has to be a one big matrix\n",
    "X_train_realised = np.stack([var[1] for var in X_train_realised.items()])\n",
    "X_train_realised = np.moveaxis(X_train_realised, 0, 1)\n",
    "\n",
    "X_val_realised = np.stack([var[1] for var in X_val_realised.items()])\n",
    "X_val_realised = np.moveaxis(X_val_realised, 0, 1)\n",
    "\n",
    "X_test_realised = np.stack([var[1] for var in X_test_realised.items()])\n",
    "X_test_realised = np.moveaxis(X_test_realised, 0, 1)\n",
    "\n",
    "print(X_train_realised.shape, X_val_realised.shape, X_test_realised.shape)\n",
    "\n",
    "# days data has to be a one big matrix\n",
    "X_train_days = np.stack([var[1] for var in X_train_days.items()])\n",
    "X_train_days = np.moveaxis(X_train_days, 0, 1)\n",
    "\n",
    "X_val_days = np.stack([var[1] for var in X_val_days.items()])\n",
    "X_val_days = np.moveaxis(X_val_days, 0, 1)\n",
    "\n",
    "X_test_days = np.stack([var[1] for var in X_test_days.items()])\n",
    "X_test_days = np.moveaxis(X_test_days, 0, 1)\n",
    "\n",
    "print(X_train_days.shape, X_val_days.shape, X_test_days.shape)\n",
    "\n",
    "# y data has to be a one big matrix\n",
    "y_train = np.stack([var[1] for var in y_train.items()])\n",
    "y_train = np.moveaxis(y_train, 0, 1)\n",
    "\n",
    "y_val = np.stack([var[1] for var in y_val.items()])\n",
    "y_val = np.moveaxis(y_val, 0, 1)\n",
    "\n",
    "y_test = np.stack([var[1] for var in y_test.items()])\n",
    "y_test = np.moveaxis(y_test, 0, 1)\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each key/value of the feature_channel/weather to a global variable\n",
    "for key,val in X_train_fc_gen.items():\n",
    "    exec(key + '=val')\n",
    "        \n",
    "for key,val in X_val_fc_gen.items():\n",
    "    exec(key + '=val')\n",
    "\n",
    "for key,val in X_test_fc_gen.items():\n",
    "    exec(key + '=val')\n",
    "    \n",
    "\n",
    "for key,val in X_train_fc_load.items():\n",
    "    exec(key + '=val')\n",
    "        \n",
    "for key,val in X_val_fc_load.items():\n",
    "    exec(key + '=val')\n",
    "\n",
    "for key,val in X_test_fc_load.items():\n",
    "    exec(key + '=val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_part_train = train_weather_gen_step_0.astype('float32')\n",
    "conv_part_val = val_weather_gen_step_0.astype('float32')\n",
    "conv_part_test = test_weather_gen_step_0.astype('float32')\n",
    "\n",
    "attn_part_train = X_train_realised.astype('float32')\n",
    "attn_part_val = X_val_realised.astype('float32')\n",
    "attn_part_test = X_test_realised.astype('float32')\n",
    "\n",
    "tabular_part_train = X_train_days.astype('float32')\n",
    "tabular_part_val = X_val_days.astype('float32')\n",
    "tabular_part_test = X_test_days.astype('float32')\n",
    "\n",
    "# target_train = y_train.reshape(-1,96*4).astype('float32')\n",
    "# target_val = y_val.reshape(-1,96*4).astype('float32')\n",
    "# target_test = y_test.astype('float32')\n",
    "\n",
    "\n",
    "n_steps_conv = len(time_steps_weather)\n",
    "\n",
    "n_steps = X_train_realised.shape[1]\n",
    "n_features = X_train_realised.shape[2]\n",
    "\n",
    "n_tab_in_steps = X_train_days.shape[1]\n",
    "n_tab_in_features = X_train_days.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframes for clarity\n",
    "\n",
    "Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['temp', 'rhum', 'prcp', 'wdir', 'wspd', 'pres', 'tsun'] [\"Wind\",\"Solar\"]\n",
    "#    0       1        2       3      4        5      6        7      8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean values are chosen for the weather data and the sum will be chosen for the installed capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train = pd.DataFrame(np.mean(conv_part_train.reshape(conv_part_train.shape[0],-1,conv_part_train.shape[3]),axis=1))\n",
    "df_weather_val = pd.DataFrame(np.mean(conv_part_val.reshape(conv_part_val.shape[0],-1,conv_part_val.shape[3]),axis=1))\n",
    "df_weather_test = pd.DataFrame(np.mean(conv_part_test.reshape(conv_part_test.shape[0],-1,conv_part_test.shape[3]),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train.rename(columns={0 : 'temp',\n",
    "                                 1 : 'rhum',\n",
    "                                 2 : 'prcp',\n",
    "                                 3 : 'wdir',\n",
    "                                 4 : 'wspd',\n",
    "                                 5 : 'pres',\n",
    "                                 6 : 'tsun',\n",
    "                                 7 : \"Wind\",\n",
    "                                 8 : \"Solar\"},inplace=True)\n",
    "\n",
    "df_weather_val.rename(columns={  0 : 'temp',\n",
    "                                 1 : 'rhum',\n",
    "                                 2 : 'prcp',\n",
    "                                 3 : 'wdir',\n",
    "                                 4 : 'wspd',\n",
    "                                 5 : 'pres',\n",
    "                                 6 : 'tsun',\n",
    "                                 7 : \"Wind\",\n",
    "                                 8 : \"Solar\"},inplace=True)\n",
    "\n",
    "df_weather_test.rename(columns={ 0 : 'temp',\n",
    "                                 1 : 'rhum',\n",
    "                                 2 : 'prcp',\n",
    "                                 3 : 'wdir',\n",
    "                                 4 : 'wspd',\n",
    "                                 5 : 'pres',\n",
    "                                 6 : 'tsun',\n",
    "                                 7 : \"Wind\",\n",
    "                                 8 : \"Solar\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_train.loc[:,\"Wind\"] = (np.sum(conv_part_train.reshape(conv_part_train.shape[0],-1,conv_part_train.shape[3]),axis=1))[:,7]\n",
    "df_weather_val.loc[:,\"Wind\"] = (np.sum(conv_part_val.reshape(conv_part_val.shape[0],-1,conv_part_val.shape[3]),axis=1))[:,7]\n",
    "df_weather_test.loc[:,\"Wind\"] = (np.sum(conv_part_test.reshape(conv_part_test.shape[0],-1,conv_part_test.shape[3]),axis=1))[:,7]\n",
    "\n",
    "df_weather_train.loc[:,\"Solar\"] = (np.sum(conv_part_train.reshape(conv_part_train.shape[0],-1,conv_part_train.shape[3]),axis=1))[:,8]\n",
    "df_weather_val.loc[:,\"Solar\"] = (np.sum(conv_part_val.reshape(conv_part_val.shape[0],-1,conv_part_val.shape[3]),axis=1))[:,7]\n",
    "df_weather_test.loc[:,\"Solar\"] = (np.sum(conv_part_test.reshape(conv_part_test.shape[0],-1,conv_part_test.shape[3]),axis=1))[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Realised/System total load in MAW', 'Realised/Wind Offshore in MAW',\n",
    "#        'Realised/Wind Onshore in MAW', 'Realised/Solar in MAW'] / [\"1lag\", \"2lag\" \"etc.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realised_train = pd.DataFrame(attn_part_train.reshape(attn_part_train.shape[0],-1))\n",
    "df_realised_val = pd.DataFrame(attn_part_val.reshape(attn_part_val.shape[0],-1))\n",
    "df_realised_test = pd.DataFrame(attn_part_test.reshape(attn_part_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(attn_part_train.shape[1]):\n",
    "    df_realised_train.rename(columns={0+i*4 : 'Realised/System total load in MAW_'+ f'lag_{i+1}',\n",
    "                                      1+i*4 : 'Realised/Wind Offshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      2+i*4 : 'Realised/Wind Onshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      3+i*4 : 'Realised/Solar in MAW_'+ f'lag_{i+1}'},inplace=True)\n",
    "    \n",
    "    df_realised_val.rename(columns={0+i*4 : 'Realised/System total load in MAW_'+ f'lag_{i+1}',\n",
    "                                      1+i*4 : 'Realised/Wind Offshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      2+i*4 : 'Realised/Wind Onshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      3+i*4 : 'Realised/Solar in MAW_'+ f'lag_{i+1}'},inplace=True)\n",
    "    \n",
    "    df_realised_test.rename(columns={0+i*4 : 'Realised/System total load in MAW_'+ f'lag_{i+1}',\n",
    "                                      1+i*4 : 'Realised/Wind Offshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      2+i*4 : 'Realised/Wind Onshore in MAW_'+ f'lag_{i+1}',\n",
    "                                      3+i*4 : 'Realised/Solar in MAW_'+ f'lag_{i+1}'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entsoe + Date +  Holiday  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabular_train = pd.DataFrame(data=tabular_part_train.reshape(-1,12),\n",
    "                                 columns = ['Day ahead/System total load in MAW',\n",
    "                                            'Day ahead/Solar in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',\n",
    "                                            'Day ahead/Wind Offshore in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday'])\n",
    "df_tabular_val = pd.DataFrame(data=tabular_part_val.reshape(-1,12),\n",
    "                               columns = ['Day ahead/System total load in MAW',\n",
    "                                            'Day ahead/Solar in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',\n",
    "                                            'Day ahead/Wind Offshore in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday'])\n",
    "df_tabular_test = pd.DataFrame(data=tabular_part_test.reshape(-1,12),\n",
    "                                 columns = ['Day ahead/System total load in MAW',\n",
    "                                            'Day ahead/Solar in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',\n",
    "                                            'Day ahead/Wind Offshore in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_MASE(y_true,y_pred):\n",
    "    overall_mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    naive_forecast = y_true[1:]\n",
    "    y_true_for_mase = y_true[:-1]\n",
    "    mae_naive = mean_absolute_error(y_true_for_mase, naive_forecast)\n",
    "    overall_mae_without_first_observation = mean_absolute_error(y_true[1:], y_pred[1:])\n",
    "\n",
    "    overall_mase = overall_mae_without_first_observation/mae_naive\n",
    "    \n",
    "    return overall_mae, overall_mase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models \n",
    "\n",
    "\n",
    "Heard from a Kaggle Grandmaster:\n",
    "\n",
    "Learning rate = 0.05, 1000 rounds, max depth = 3-5, subsample = 0.8-1.0, colsample_bytree = 0.3 - 0.8, lambda = 0 to 5\n",
    "\n",
    "Add capacity to combat bias - add rounds\n",
    "\n",
    "Reduce capacity to combat variance - depth / regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "First iteration:\n",
    "['temp', 'prcp', 'pres', 'tsun'] + [all lags load] + [load entsoe, all dates, holliday] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_w_train = df_weather_train.loc[:,['temp', 'prcp', 'pres', 'tsun']]\n",
    "df_load_w_val = df_weather_val.loc[:,['temp', 'prcp', 'pres', 'tsun']]\n",
    "df_load_w_test = df_weather_test.loc[:,['temp', 'prcp', 'pres', 'tsun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_lag_train = df_realised_train.filter(regex='load')\n",
    "df_load_lag_val = df_realised_val.filter(regex='load')\n",
    "df_load_lag_test = df_realised_test.filter(regex='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_tab_train = df_tabular_train.loc[:,['Day ahead/System total load in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday']]\n",
    "\n",
    "df_load_tab_val = df_tabular_val.loc[:,['Day ahead/System total load in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday']]\n",
    "\n",
    "df_load_tab_test = df_tabular_test.loc[:,['Day ahead/System total load in MAW',\n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month',\n",
    "                                            'sin_weekday', \n",
    "                                            'cos_weekday',\n",
    "                                            'holliday']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_train = pd.concat([df_load_w_train, df_load_lag_train, df_load_tab_train], axis=1)\n",
    "df_load_val = pd.concat([df_load_w_val, df_load_lag_val, df_load_tab_val], axis=1)\n",
    "df_load_test = pd.concat([df_load_w_test, df_load_lag_test, df_load_tab_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target distribution in y_train is:\n",
    "\n",
    "'Realised/System total load in MAW', 'Realised/Wind Offshore in MAW','Realised/Wind Onshore in MAW', 'Realised/Solar in MAW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_load_train = y_train.reshape(y_train.shape[0],-1)[:,0]\n",
    "target_load_val = y_val.reshape(y_val.shape[0],-1)[:,0] \n",
    "target_load_test = y_test.reshape(y_test.shape[0],-1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xgb = xgbr(n_jobs=-1)\n",
    "# load_xgb = xgbr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='XGB Load',\n",
    "#                  n_jobs=-1, colsample_by_tree=0.8, reg_lambda=1#, random_state \n",
    "#                 )\n",
    "\n",
    "\n",
    "load_brf = xgbrfr(n_jobs=-1)\n",
    "# load_brf = xgbrfr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='BRF Load',\n",
    "#                   n_jobs=-1, colsample_bytree=0.8, reg_lambda=1, #random_state\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjarke/anaconda3/envs/general/lib/python3.7/site-packages/xgboost/core.py:613: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:37:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "load_xgb = load_xgb.fit(df_load_train,target_load_train)\n",
    "load_brf = load_brf.fit(df_load_train,target_load_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xgb_pred = load_xgb.predict(df_load_test)\n",
    "load_brf_pred = load_brf.predict(df_load_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Load XGB\n",
      "The overall mean absolute error of the model in MW is: 371.11896874276283\n",
      "The overall mean absolute error of the model in MW is: 0.7024875014235272\n",
      "=================================\n",
      "=================================\n",
      "Load BRF\n",
      "The overall mean absolute error of the model in MW is: 1161.304232916715\n",
      "The overall mean absolute error of the model in MW is: 2.1979977268296884\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=================================\")\n",
    "print(\"Load XGB\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_load_test,load_xgb_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Load BRF\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_load_test,load_brf_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Wind\n",
    "\n",
    "First iteration:\n",
    "['prcp', 'wdir', 'wspd', 'pres', 'Wind'] + [all lags wind gen] + [both wind entsoe, year, minute, month] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_w_train = df_weather_train.loc[:,['prcp', 'wdir', 'wspd', 'pres', 'Wind']]\n",
    "df_wind_w_val = df_weather_val.loc[:,['prcp', 'wdir', 'wspd', 'pres', 'Wind']]\n",
    "df_wind_w_test = df_weather_test.loc[:,['prcp', 'wdir', 'wspd', 'pres', 'Wind']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_lag_train = df_realised_train.filter(regex='Wind')\n",
    "df_wind_lag_val = df_realised_val.filter(regex='Wind')\n",
    "df_wind_lag_test = df_realised_test.filter(regex='Wind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_tab_train = df_tabular_train.loc[:,['Day ahead/Wind Offshore in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',                                            \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]\n",
    "\n",
    "df_wind_tab_val = df_tabular_val.loc[:,['Day ahead/Wind Offshore in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',                                            \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]\n",
    "\n",
    "df_wind_tab_test = df_tabular_test.loc[:,['Day ahead/Wind Offshore in MAW',\n",
    "                                            'Day ahead/Wind Onshore in MAW',                                            \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind_train = pd.concat([df_wind_w_train, df_wind_lag_train, df_wind_tab_train], axis=1)\n",
    "df_wind_val = pd.concat([df_wind_w_val, df_wind_lag_val, df_wind_tab_val], axis=1)\n",
    "df_wind_test = pd.concat([df_wind_w_test, df_wind_lag_test, df_wind_tab_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_wind_off_train = y_train.reshape(y_train.shape[0],-1)[:,1]\n",
    "target_wind_off_val = y_val.reshape(y_val.shape[0],-1)[:,1] \n",
    "target_wind_off_test = y_test.reshape(y_test.shape[0],-1)[:,1]\n",
    "\n",
    "target_wind_on_train = y_train.reshape(y_train.shape[0],-1)[:,2]\n",
    "target_wind_on_val = y_val.reshape(y_val.shape[0],-1)[:,2] \n",
    "target_wind_on_test = y_test.reshape(y_test.shape[0],-1)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_off_xgb = xgbr(n_jobs=-1)\n",
    "# wind_off_xgb = xgbr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='XGB Wind_off',\n",
    "#                  n_jobs=-1, colsample_by_tree=0.8, reg_lambda=1#, random_state \n",
    "#                 )\n",
    "\n",
    "wind_off_brf = xgbrfr(n_jobs=-1)\n",
    "# wind_off_brf = xgbrfr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='BRF Wind_off',\n",
    "#                   n_jobs=-1, colsample_bytree=0.8, reg_lambda=1, #random_state\n",
    "#                  )\n",
    "\n",
    "wind_on_xgb = xgbr(n_jobs=-1)\n",
    "# wind_on_xgb = xgbr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='XGB Wind_on',\n",
    "#                  n_jobs=-1, colsample_by_tree=0.8, reg_lambda=1#, random_state \n",
    "#                 )\n",
    "\n",
    "wind_on_brf = xgbrfr(n_jobs=-1)\n",
    "# wind_on_brf = xgbrfr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='BRF Wind_on',\n",
    "#                   n_jobs=-1, colsample_bytree=0.8, reg_lambda=1, #random_state\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjarke/anaconda3/envs/general/lib/python3.7/site-packages/xgboost/core.py:613: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:37:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:37:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:37:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "wind_off_xgb = wind_off_xgb.fit(df_wind_train,target_wind_off_train)\n",
    "wind_off_brf = wind_off_brf.fit(df_wind_train,target_wind_off_train)\n",
    "\n",
    "wind_on_xgb = wind_on_xgb.fit(df_wind_train,target_wind_on_train)\n",
    "wind_on_brf = wind_on_brf.fit(df_wind_train,target_wind_on_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_off_xgb_pred = wind_off_xgb.predict(df_wind_test)\n",
    "wind_off_brf_pred = wind_off_brf.predict(df_wind_test)\n",
    "\n",
    "wind_on_xgb_pred = wind_on_xgb.predict(df_wind_test)\n",
    "wind_on_brf_pred = wind_on_brf.predict(df_wind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Wind off XGB\n",
      "The overall mean absolute error of the model in MW is: 83.62393423383465\n",
      "The overall mean absolute error of the model in MW is: 0.965558616239794\n",
      "=================================\n",
      "=================================\n",
      "Wind off BRF\n",
      "The overall mean absolute error of the model in MW is: 209.78118123473698\n",
      "The overall mean absolute error of the model in MW is: 2.4221243631535665\n",
      "=================================\n",
      "=================================\n",
      "Wind on XGB\n",
      "The overall mean absolute error of the model in MW is: 185.0323199527912\n",
      "The overall mean absolute error of the model in MW is: 0.9217270188444872\n",
      "=================================\n",
      "=================================\n",
      "Wind on BRF\n",
      "The overall mean absolute error of the model in MW is: 889.9912496392735\n",
      "The overall mean absolute error of the model in MW is: 4.433066927901632\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=================================\")\n",
    "print(\"Wind off XGB\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_wind_off_test,wind_off_xgb_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Wind off BRF\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_wind_off_test,wind_off_brf_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"Wind on XGB\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_wind_on_test,wind_on_xgb_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Wind on BRF\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_wind_on_test,wind_on_brf_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Solar\n",
    "\n",
    "First iteration:\n",
    "['temp', 'rhum', 'prcp', 'pres', 'tsun', 'Solar'] + [all lags solar gen] + [solar entsoe, year, minute, month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_w_train = df_weather_train.loc[:,['temp', 'rhum', 'prcp', 'pres', 'tsun', 'Solar']]\n",
    "df_solar_w_val = df_weather_val.loc[:,['temp', 'rhum', 'prcp', 'pres', 'tsun', 'Solar']]\n",
    "df_solar_w_test = df_weather_test.loc[:,['temp', 'rhum', 'prcp', 'pres', 'tsun', 'Solar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_lag_train = df_realised_train.filter(regex='Solar')\n",
    "df_solar_lag_val = df_realised_val.filter(regex='Solar')\n",
    "df_solar_lag_test = df_realised_test.filter(regex='Solar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_tab_train = df_tabular_train.loc[:,['Day ahead/Solar in MAW',                                            \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]\n",
    "\n",
    "df_solar_tab_val = df_tabular_val.loc[:,['Day ahead/Solar in MAW',                                            \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]\n",
    "\n",
    "df_solar_tab_test = df_tabular_test.loc[:,['Day ahead/Solar in MAW',                                           \n",
    "                                            'Year',\n",
    "                                            'sin_minute',\n",
    "                                            'cos_minute',\n",
    "                                            'sin_month',\n",
    "                                            'cos_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_train = pd.concat([df_solar_w_train, df_solar_lag_train, df_solar_tab_train], axis=1)\n",
    "df_solar_val = pd.concat([df_solar_w_val, df_solar_lag_val, df_solar_tab_val], axis=1)\n",
    "df_solar_test = pd.concat([df_solar_w_test, df_solar_lag_test, df_solar_tab_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_solar_train = y_train.reshape(y_train.shape[0],-1)[:,3]\n",
    "target_solar_val = y_val.reshape(y_val.shape[0],-1)[:,3] \n",
    "target_solar_test = y_test.reshape(y_test.shape[0],-1)[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_xgb = xgbr(n_jobs=-1)\n",
    "# solar_xgb = xgbr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='XGB Solar',\n",
    "#                  n_jobs=-1, colsample_by_tree=0.8, reg_lambda=1#, random_state \n",
    "#                 )\n",
    "\n",
    "solar_brf = xgbrfr(n_jobs=-1)\n",
    "# solar_brf = xgbrfr(n_estimators=1000, max_depth=3, learning_rate=0.05, objectve='BRF Solar',\n",
    "#                   n_jobs=-1, colsample_bytree=0.8, reg_lambda=1, #random_state\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjarke/anaconda3/envs/general/lib/python3.7/site-packages/xgboost/core.py:613: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\"Use subset (sliced data) of np.ndarray is not recommended \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "solar_xgb = solar_xgb.fit(df_solar_train,target_solar_train)\n",
    "solar_brf = solar_brf.fit(df_solar_train,target_solar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_xgb_pred = solar_xgb.predict(df_solar_test)\n",
    "solar_brf_pred = solar_brf.predict(df_solar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Solar XGB\n",
      "The overall mean absolute error of the model in MW is: 154.81323570013046\n",
      "The overall mean absolute error of the model in MW is: 0.3789127384974852\n",
      "=================================\n",
      "=================================\n",
      "Solar BRF\n",
      "The overall mean absolute error of the model in MW is: 563.3137349530247\n",
      "The overall mean absolute error of the model in MW is: 1.378724905969945\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=================================\")\n",
    "print(\"Solar XGB\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_solar_test,solar_xgb_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Solar BRF\")\n",
    "overall_mae, overall_mase = MAE_MASE(target_solar_test,solar_brf_pred)\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mae}\")\n",
    "print(f\"The overall mean absolute error of the model in MW is: {overall_mase}\")\n",
    "print(\"=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
